{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer as Tfidf\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier as dt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split as sk_split\n",
    "from sklearn.decomposition import TruncatedSVD as SVD\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll read the raw reviews from our Yelp JSON dataset, flatten the \"votes\" subobject into \"cool\" \"funny\" and \"useful\" attributes, then cast it to a pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('reviews.json') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "reviews = []\n",
    "for rev in data:\n",
    "    rev['cool'] = rev['votes']['cool']\n",
    "    rev['funny'] = rev['votes']['funny']\n",
    "    rev['useful'] = rev['votes']['useful']\n",
    "    del rev['votes']\n",
    "    reviews.append(rev)\n",
    "    \n",
    "reviews = pd.DataFrame(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to clean the reviews by removing duplicates. After doing so, we'll extract all reviews made during 2015 to comprise our model's sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews.duplicated(subset=['business_id', 'text', 'date', 'user_id'])\n",
    "before = reviews.shape[0]\n",
    "reviews = reviews.drop(reviews[reviews.duplicated(subset=['business_id', 'text', 'date', 'user_id'])].index)\n",
    "after = reviews.shape[0]\n",
    "\n",
    "print before - after, \"duplicates dropped\"\n",
    "\n",
    "reviews_2015 = reviews[(reviews['date'] >= '2015-01-01') & (reviews['date'] < '2016-01-01')]\n",
    "del reviews_2015['review_id']\n",
    "del reviews_2015['type']\n",
    "\n",
    "print reviews_2015.shape\n",
    "reviews_2015.to_json(\"json_2015.json\", orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly inspect the data to ensure that everything looks right. Note that the CSV read/write operation is in place because the notebook was originally split into a cleaning notebook and a modeling notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vT12uXtdBQ10_lUcl-M40w</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>terrible experience....I am a local first of a...</td>\n",
       "      <td>0</td>\n",
       "      <td>C_xtIn19eKivN335dzjadg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aWMxTWSEqBvH2KhdGPLibQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Bad service at this location.\\n\\nGood iced cof...</td>\n",
       "      <td>0</td>\n",
       "      <td>T7J9ae0wTskrI_Bgwp-4cA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Z98h1BhssZeFfZvcVTYOpw</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>We have been to Maya's many times since it fir...</td>\n",
       "      <td>1</td>\n",
       "      <td>O7WaUuYwX45Ia6Mvf01UCw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3rwM9fPYPk9qDkEBOhyHbg</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>booked the hotel for new years, over the phone...</td>\n",
       "      <td>1</td>\n",
       "      <td>kq-4vbC1cHQbRKyDmwERSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JpYn_HdxQNZQSlWWv4P6Iw</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Good, not great. Not a ton of tea options but ...</td>\n",
       "      <td>1</td>\n",
       "      <td>XweddetOpWNuJQ5mLb_5JQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool       date  funny  stars  \\\n",
       "0  vT12uXtdBQ10_lUcl-M40w     0 2015-01-01      0      1   \n",
       "1  aWMxTWSEqBvH2KhdGPLibQ     0 2015-01-01      0      3   \n",
       "2  Z98h1BhssZeFfZvcVTYOpw     0 2015-01-01      0      2   \n",
       "3  3rwM9fPYPk9qDkEBOhyHbg     1 2015-01-01      0      5   \n",
       "4  JpYn_HdxQNZQSlWWv4P6Iw     0 2015-01-01      0      3   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  terrible experience....I am a local first of a...       0   \n",
       "1  Bad service at this location.\\n\\nGood iced cof...       0   \n",
       "2  We have been to Maya's many times since it fir...       1   \n",
       "3  booked the hotel for new years, over the phone...       1   \n",
       "4  Good, not great. Not a ton of tea options but ...       1   \n",
       "\n",
       "                  user_id  \n",
       "0  C_xtIn19eKivN335dzjadg  \n",
       "1  T7J9ae0wTskrI_Bgwp-4cA  \n",
       "2  O7WaUuYwX45Ia6Mvf01UCw  \n",
       "3  kq-4vbC1cHQbRKyDmwERSA  \n",
       "4  XweddetOpWNuJQ5mLb_5JQ  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json('json_2015.json')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Let's sample from this subset of 2015 reviews so that our model is a bit easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samp = data.sample(50000) #draw samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now establish our baseline model. For this we'll use a very basic vectorizer with no weighting or other such modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', binary=False, max_features=10000) #using non-binary Count Vec.\n",
    "reviews = samp.text.values\n",
    "\n",
    "#tokenize words\n",
    "x = vectorizer.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting response variable for \"useful\" to binary form. In other words, reviews with at least one useful vote are labeled as \"1\" while those without are labeled as \"0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary_y = [] #class observations according to whether they have at least one \"helpful\" vote\n",
    "\n",
    "for score in samp.useful.values:\n",
    "    i = 1 if score > 0 else 0\n",
    "    binary_y.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the optimal logistic regression regularization parameter for this dataset. It appears to be about 0.01, further tinkering yielded no additional gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C= 0.0001 : 0.646140014067\n",
      "C= 0.001 : 0.662779856527\n",
      "C= 0.01 : 0.66393987653\n",
      "C= 0.1 : 0.653520002091\n",
      "C= 1 : 0.627160042005\n",
      "C= 10 : 0.606280070737\n",
      "C= 100 : 0.600379882726\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEdCAYAAAAikTHKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3fb812550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cvals = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "scores = []\n",
    "for c in cvals: #try cross validating the data with different regularization parameters\n",
    "    log_model = LogReg(C=c, penalty='l2') #initiate unregularized logistic regression model\n",
    "    score = cross_val_score(log_model, x, binary_y, n_jobs=-1).mean() #cross validate\n",
    "    scores.append(score)\n",
    "    print \"C=\",c ,\":\", score\n",
    "    \n",
    "    \n",
    "plt.plot(cvals, scores)\n",
    "plt.xscale('log')\n",
    "plt.title(\"Accuracy by Regularization Parameter value\")\n",
    "plt.xlabel(\"Parameter value\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validated Accuracy on Sample: 0.66393987653\n"
     ]
    }
   ],
   "source": [
    "log_model = LogReg(C=0.01, penalty='l2') #initialize logistic regression model\n",
    "print \"Cross Validated Accuracy on Sample:\", cross_val_score(log_model, x, binary_y, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, not a bad number. However, we should look at how this model fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO CLASS BALANCE CORRECTION\n",
      "[[28903  2247]\n",
      " [12761  6089]]\n",
      "\n",
      "False Positive Rate: 0.0721348314607\n",
      "False Negative Rate: 0.676976127321\n",
      "True Positive Rate: 0.730446257198\n",
      "True Negative Rate: 0.693716397849\n",
      "\n",
      "Accuracy on Positive Reviews: 0.323023872679\n",
      "Accuracy on Negative Reviews: 0.927865168539\n",
      "Overall F1 Score: 0.377639697816\n",
      "\n",
      "\n",
      "WITH CLASS BALANCE CORRECTION\n",
      "[[24119  7031]\n",
      " [ 8301 10549]]\n",
      "\n",
      "False Positive Rate: 0.225714285714\n",
      "False Negative Rate: 0.440371352785\n",
      "True Positive Rate: 0.600056882821\n",
      "True Negative Rate: 0.743954349167\n",
      "\n",
      "Accuracy on Positive Reviews: 0.559628647215\n",
      "Accuracy on Negative Reviews: 0.774285714286\n",
      "Overall F1 Score: 0.512523895903\n"
     ]
    }
   ],
   "source": [
    "log_model = LogReg(C=0.01, penalty='l2') #initialize logistic regression model\n",
    "log_model.fit(x, binary_y)\n",
    "y_pred = log_model.predict(x)\n",
    "binary_y = np.array(binary_y)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(binary_y, y_pred).ravel() #from sklearn docs\n",
    "\n",
    "print \"NO CLASS BALANCE CORRECTION\"\n",
    "print confusion_matrix(binary_y, y_pred)\n",
    "print\n",
    "print \"False Positive Rate:\", fp/float(fp+tn)\n",
    "print \"False Negative Rate:\", fn/float(fn+tp)\n",
    "print \"True Positive Rate:\", tp/float(tp+fp)\n",
    "print \"True Negative Rate:\", tn/float(tn+fn)\n",
    "print\n",
    "print \"Accuracy on Positive Reviews:\", np.mean(y_pred[binary_y == 1] == binary_y[binary_y == 1])\n",
    "print \"Accuracy on Negative Reviews:\", np.mean(y_pred[binary_y == 0] == binary_y[binary_y == 0])\n",
    "\n",
    "print \"Overall F1 Score:\", cross_val_score(log_model, x, binary_y, n_jobs=-1, scoring='f1').mean()\n",
    "\n",
    "#BALANCED CLASSES\n",
    "log_model = LogReg(C=0.01, penalty='l2', class_weight='balanced') #initialize logistic regression model\n",
    "log_model.fit(x, binary_y)\n",
    "y_pred = log_model.predict(x)\n",
    "binary_y = np.array(binary_y)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(binary_y, y_pred).ravel() #from sklearn docs\n",
    "\n",
    "print \"\\n\\nWITH CLASS BALANCE CORRECTION\"\n",
    "print confusion_matrix(binary_y, y_pred)\n",
    "print\n",
    "print \"False Positive Rate:\", fp/float(fp+tn)\n",
    "print \"False Negative Rate:\", fn/float(fn+tp)\n",
    "print \"True Positive Rate:\", tp/float(tp+fp)\n",
    "print \"True Negative Rate:\", tn/float(tn+fn)\n",
    "print\n",
    "print \"Accuracy on Positive Reviews:\", np.mean(y_pred[binary_y == 1] == binary_y[binary_y == 1])\n",
    "print \"Accuracy on Negative Reviews:\", np.mean(y_pred[binary_y == 0] == binary_y[binary_y == 0])\n",
    "\n",
    "print \"Overall F1 Score:\", cross_val_score(log_model, x, binary_y, n_jobs=-1, scoring='f1').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the highly imbalanced classes were the cause for our high initial cross validated accuracy. Weighting improves our accuracy on the positive reviews, but the model is still relatively weak. We'll look at some more advanced techniques for improving overall accuracy and positive accuracy rate after running the baseline algorithm on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#same procedure on full dataset\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', binary=False, max_features=10000) #using non-binary Count Vec.\n",
    "reviews = data.text.values\n",
    "x = vectorizer.fit_transform(reviews)\n",
    "\n",
    "binary_y = []\n",
    "\n",
    "for score in data.useful.values:\n",
    "    i = 1 if score > 0 else 0\n",
    "    binary_y.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.655814588905\n",
      "F1 Score: 0.525511083732\n",
      "\n",
      "\n",
      "WITH CLASS BALANCE CORRECTION\n",
      "[[314277 101194]\n",
      " [119596 131748]]\n",
      "\n",
      "False Positive Rate: 0.243564532783\n",
      "False Negative Rate: 0.475825959641\n",
      "True Positive Rate: 0.56558284895\n",
      "True Negative Rate: 0.724352517903\n",
      "\n",
      "Accuracy on Positive Reviews: 0.524174040359\n",
      "Accuracy on Negative Reviews: 0.756435467217\n",
      "Overall F1 Score: 0.525511083732\n"
     ]
    }
   ],
   "source": [
    "log_model = LogReg(C=0.01, penalty='l2', class_weight='balanced') #initialize logistic regression model\n",
    "print \"Accuracy:\", cross_val_score(log_model, x, binary_y, n_jobs=-1).mean()\n",
    "print \"F1 Score:\",cross_val_score(log_model, x, binary_y, scoring='f1', n_jobs=-1).mean()\n",
    "\n",
    "log_model.fit(x, binary_y)\n",
    "y_pred = log_model.predict(x)\n",
    "binary_y = np.array(binary_y)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(binary_y, y_pred).ravel() #from sklearn docs\n",
    "\n",
    "print \"\\n\\nWITH CLASS BALANCE CORRECTION\"\n",
    "print confusion_matrix(binary_y, y_pred)\n",
    "print\n",
    "print \"False Positive Rate:\", fp/float(fp+tn)\n",
    "print \"False Negative Rate:\", fn/float(fn+tp)\n",
    "print \"True Positive Rate:\", tp/float(tp+fp)\n",
    "print \"True Negative Rate:\", tn/float(tn+fn)\n",
    "print\n",
    "print \"Accuracy on Positive Reviews:\", np.mean(y_pred[binary_y == 1] == binary_y[binary_y == 1])\n",
    "print \"Accuracy on Negative Reviews:\", np.mean(y_pred[binary_y == 0] == binary_y[binary_y == 0])\n",
    "\n",
    "print \"Overall F1 Score:\", cross_val_score(log_model, x, binary_y, n_jobs=-1, scoring='f1').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, while our baseline model establishes some degree of separation between helpful and unhelpful reviews it seems to have a relatively low accuracy and F1 score even when tuned. This is likely due to a confounding element of our question: the reality that reviews can lack \"useful\" votes for multiple reasons. \n",
    "\n",
    "On one hand, a review can have no useful votes when it actually contains useless content. This is the sort of review we'd like to classify as 0 or useless. On the other hand, a great review can recieve no useful votes simply because it goes unnoticed by other Yelp users. This likely impedes our model's capacity to make solid predictions regarding a review's helpfulness. A \"denominator\" value such as the sort seen in the Amazon dataset we looked at previously would help alleviate this issue.\n",
    "\n",
    "Given this difficulty, it will make sense for us to fit some additional models to the data to see which works best. Playing with class weights would also make sense, as the model currently seems to be making a fair number of false negative errors (which are severely impacting its performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('json_2015.json')\n",
    "data = data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>535690</th>\n",
       "      <td>mTLxc3DqWqFk0D2mijfuBw</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-10-18</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Went here with boyfriend last night at his rec...</td>\n",
       "      <td>1</td>\n",
       "      <td>sbmv5VijXRk6Hc1tZXWTfw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17945</th>\n",
       "      <td>_PuhheIPgmDCBJe1nhpKTg</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Looking for a good Italian place in Vegas and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>E6MDZx4Eh3JZzyL6J8DPKQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468801</th>\n",
       "      <td>dz1bfyHR5lOdse4b4UiQiQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-09-12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ordered a sandwich which they forgot to put sa...</td>\n",
       "      <td>2</td>\n",
       "      <td>4ltmmYvQTtS3Um3UWvjBRg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603586</th>\n",
       "      <td>MmNuDDHUHDgLj4BviOvlqg</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-11-24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>After years of wanting to replace my worn down...</td>\n",
       "      <td>0</td>\n",
       "      <td>yWaoi5rHDb5b8Lzzh5tJqw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184502</th>\n",
       "      <td>dVk_hKqA4mi0XOJq4pfJiQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-04-18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Not impressed. Yeah having the free vacuums is...</td>\n",
       "      <td>5</td>\n",
       "      <td>NSNyXuGQXpvkpvXqPR7Yjw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   business_id  cool       date  funny  stars  \\\n",
       "535690  mTLxc3DqWqFk0D2mijfuBw     0 2015-10-18      0      3   \n",
       "17945   _PuhheIPgmDCBJe1nhpKTg     0 2015-01-11      1      5   \n",
       "468801  dz1bfyHR5lOdse4b4UiQiQ     0 2015-09-12      0      1   \n",
       "603586  MmNuDDHUHDgLj4BviOvlqg     0 2015-11-24      0      1   \n",
       "184502  dVk_hKqA4mi0XOJq4pfJiQ     0 2015-04-18      1      1   \n",
       "\n",
       "                                                     text  useful  \\\n",
       "535690  Went here with boyfriend last night at his rec...       1   \n",
       "17945   Looking for a good Italian place in Vegas and ...       0   \n",
       "468801  Ordered a sandwich which they forgot to put sa...       2   \n",
       "603586  After years of wanting to replace my worn down...       0   \n",
       "184502  Not impressed. Yeah having the free vacuums is...       5   \n",
       "\n",
       "                       user_id  \n",
       "535690  sbmv5VijXRk6Hc1tZXWTfw  \n",
       "17945   E6MDZx4Eh3JZzyL6J8DPKQ  \n",
       "468801  4ltmmYvQTtS3Um3UWvjBRg  \n",
       "603586  yWaoi5rHDb5b8Lzzh5tJqw  \n",
       "184502  NSNyXuGQXpvkpvXqPR7Yjw  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below are scorers which calculate our model's accuracy on the different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#standardize predictor variables\n",
    "\n",
    "def scorer_pos(estimator, X, y): #custom scoring functions to get positive and negative accuracy\n",
    "    estimator.fit(X, y)\n",
    "    y_pred = estimator.predict(X)\n",
    "    return np.float(np.mean(y_pred[y == 1] == y[y == 1]))\n",
    "\n",
    "def scorer_neg(estimator, X, y):\n",
    "    estimator.fit(X, y)\n",
    "    y_pred = estimator.predict(X)\n",
    "    return np.float(np.mean(y_pred[y == 0] == y[y == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function tests a model and returns a wide variety of metrics on its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(x, binary_y, model, title, coefs=False):\n",
    "    model.fit(x, binary_y)\n",
    "    y_pred = model.predict(x)\n",
    "    binary_y = np.array(binary_y)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(binary_y, y_pred).ravel() #from sklearn docs\n",
    "\n",
    "    print title\n",
    "    print confusion_matrix(binary_y, y_pred)\n",
    "\n",
    "    print \"False Positive Rate:\", fp/float(fp+tn)\n",
    "    print \"False Negative Rate:\", fn/float(fn+tp)\n",
    "    print \"True Positive Rate:\", tp/float(tp+fp)\n",
    "    print \"True Negative Rate:\", tn/float(tn+fn)\n",
    "    \n",
    "    print \"Positive Accuracy:\", cross_val_score(model, x, binary_y, n_jobs=-1, scoring=scorer_pos).mean()\n",
    "    print \"Negative Accuracy:\", cross_val_score(model, x, binary_y, n_jobs=-1, scoring=scorer_neg).mean()\n",
    "    print \"Cross Validated Accuracy on Sample:\", cross_val_score(model, x, binary_y, n_jobs=-1).mean()\n",
    "    print \"Cross Validated AUC on Sample:\", cross_val_score(model, x, binary_y, n_jobs=-1, scoring='roc_auc').mean()\n",
    "    print \"Train Set Accuracy on Sample:\", np.mean(y_pred == binary_y)\n",
    "    print\n",
    "    if coefs == True:\n",
    "        mydict = zip(model.coef_[0], vectorizer.get_feature_names())\n",
    "        words = sorted([(i[0], i[1].encode('utf-8')) for i in mydict], reverse=True, key=lambda x: x[0])\n",
    "        return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a stemmer drawn from NLTK. We found that it did not noticably increase our model's performace, so we did not end up using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we sample 100,000 reviews from the dataset and run them through a vectorizer using Tf-idf weighting and a non-binary counting mechanism. Document frequency maxima and minima were found to have little effect, and our tokenization process actually worked against us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samp = data.sample(100000) #draw samples\n",
    "\n",
    "#lengths = [len(i[1]['text']) for i in samp.iterrows()]\n",
    "\n",
    "samp.shape\n",
    "frac_useful = 1 - data['useful'].value_counts()[0] / float(data.shape[0])\n",
    "frac_funny = 1 - data['funny'].value_counts()[0] / float(data.shape[0])\n",
    "frac_cool = 1 - data['cool'].value_counts()[0] / float(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             binary=False, \n",
    "                             max_features=10000,\n",
    "                             analyzer='word',\n",
    "                             #tokenizer=tokenize,\n",
    "                             sublinear_tf=False\n",
    "                            ) #using non-binary Count Vec.\n",
    "reviews = samp.text.values\n",
    "\n",
    "#tokenize words\n",
    "x = vectorizer.fit_transform(reviews)\n",
    "\n",
    "y_useful = [] #class observations according to whether they have at least one \"helpful\" vote\n",
    "y_cool = []\n",
    "y_funny = []\n",
    "\n",
    "\n",
    "for score in samp.useful.values:\n",
    "    i = 1 if score > 0 else 0\n",
    "    y_useful.append(i)\n",
    "\n",
    "for score in samp.cool.values:\n",
    "    i = 1 if score > 0 else 0\n",
    "    y_cool.append(i)\n",
    "\n",
    "for score in samp.funny.values:\n",
    "    i = 1 if score > 0 else 0\n",
    "    y_funny.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well our algorithm does with these new vectorization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Useful\n",
      "[[38773 23462]\n",
      " [14005 23760]]\n",
      "False Positive Rate: 0.376990439463\n",
      "False Negative Rate: 0.370846021448\n",
      "True Positive Rate: 0.503155308966\n",
      "True Negative Rate: 0.734643222555\n",
      "Positive Accuracy: 0.64498863211\n",
      "Negative Accuracy: 0.611504780268\n",
      "Cross Validated Accuracy on Sample: 0.612610008022\n",
      "Cross Validated AUC on Sample: 0.654538277018\n",
      "Train Set Accuracy on Sample: 0.62533\n",
      "\n",
      "Funny\n",
      "[[52847 29821]\n",
      " [ 5835 11497]]\n",
      "False Positive Rate: 0.360732084966\n",
      "False Negative Rate: 0.336660512347\n",
      "True Positive Rate: 0.278256449973\n",
      "True Negative Rate: 0.900565761221\n",
      "Positive Accuracy: 0.678975007399\n",
      "Negative Accuracy: 0.639388880825\n",
      "Cross Validated Accuracy on Sample: 0.632620023926\n",
      "Cross Validated AUC on Sample: 0.681813486942\n",
      "Train Set Accuracy on Sample: 0.64344\n",
      "\n",
      "Cool\n",
      "[[52000 26285]\n",
      " [ 8740 12975]]\n",
      "False Positive Rate: 0.335760362777\n",
      "False Negative Rate: 0.402486760304\n",
      "True Positive Rate: 0.330489047376\n",
      "True Negative Rate: 0.856108001317\n",
      "Positive Accuracy: 0.603914495927\n",
      "Negative Accuracy: 0.671316344127\n",
      "Cross Validated Accuracy on Sample: 0.636830043527\n",
      "Cross Validated AUC on Sample: 0.65375386429\n",
      "Train Set Accuracy on Sample: 0.64975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "u_tune = 0.00\n",
    "f_tune = 0.00\n",
    "c_tune = 0.00\n",
    "\n",
    "weights = {0:1/(1 - frac_useful + u_tune), 1:1/(frac_useful - u_tune)}\n",
    "log_model = LogReg(C=0.01, penalty='l2', class_weight=weights) #initialize logistic regression model\n",
    "\n",
    "rand = RandomForestClassifier(n_estimators=300, criterion='gini', \n",
    "                              max_features= 17, max_depth=5, class_weight='balanced')\n",
    "\n",
    "\n",
    "\n",
    "useful_words = test_model(x, y_useful, \n",
    "                          log_model,\n",
    "                          \"Useful\",\n",
    "                          coefs=True)\n",
    "funny_words = test_model(x, y_funny, \n",
    "                         log_model.set_params(class_weight={0:1/(1 - frac_funny + f_tune), 1:1/(frac_funny - f_tune)}), \n",
    "                         \"Funny\",\n",
    "                          coefs=True)\n",
    "cool_words = test_model(x, y_cool, \n",
    "                        log_model.set_params(class_weight={0:1/(1 - frac_cool + c_tune), 1:1/(frac_cool - c_tune)}),\n",
    "                        #rand,\n",
    "                        \"Cool\",\n",
    "                          coefs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stability of our accuracy numbers has increased, and we see a significant increase in positive class accuracy relative to our baseline model. We found that further biasing the weights in favor of positive classifications caused large drops in negative accuracy, as per our website.\n",
    "\n",
    "Below are printouts of the 10 highest coefficient words for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Coolest Words:\n",
      "like 0.807612007641\n",
      "located 0.670322339945\n",
      "just 0.626328926314\n",
      "really 0.616388983954\n",
      "pretty 0.592052005026\n",
      "vegas 0.567971734863\n",
      "perfect 0.566693403885\n",
      "ll 0.559683702614\n",
      "super 0.528248961115\n",
      "cream 0.524181983647\n",
      "\n",
      "10 Least Cool Words:\n",
      "atmosphere -0.369284364266\n",
      "told -0.384512134228\n",
      "recommend -0.3966567704\n",
      "waited -0.397704357381\n",
      "horrible -0.399269095416\n",
      "terrible -0.400473119824\n",
      "worst -0.473362959253\n",
      "rude -0.482382202353\n",
      "service -0.592469353197\n",
      "food -0.726009765796\n",
      "\n",
      "10 Funniest Words:\n",
      "like 1.27323255495\n",
      "just 0.869431814704\n",
      "don 0.851679103682\n",
      "want 0.731578810339\n",
      "people 0.656238801864\n",
      "review 0.623923140589\n",
      "said 0.623217516443\n",
      "know 0.573615927835\n",
      "bar 0.564101760505\n",
      "thing 0.555996253301\n",
      "\n",
      "10 Least Funny Words:\n",
      "staff -0.54322547621\n",
      "excellent -0.603172586984\n",
      "delicious -0.617626437128\n",
      "love -0.61820671326\n",
      "amazing -0.666508852385\n",
      "food -0.713787452365\n",
      "best -0.722401827574\n",
      "recommend -0.752577102822\n",
      "friendly -0.81210679357\n",
      "great -1.30580185234\n",
      "\n",
      "10 Most Useful Words:\n",
      "like 0.811041910952\n",
      "just 0.724975681659\n",
      "don 0.666030537768\n",
      "said 0.596250526402\n",
      "review 0.519063167632\n",
      "know 0.504829163478\n",
      "think 0.493770615359\n",
      "ll 0.493168335011\n",
      "bar 0.487582234473\n",
      "want 0.486004886789\n",
      "\n",
      "10 Least Useful Words:\n",
      "friendly -0.414232456561\n",
      "recommend -0.422263324432\n",
      "excellent -0.454653673375\n",
      "amazing -0.476685260294\n",
      "best -0.488726126999\n",
      "awesome -0.521324185133\n",
      "service -0.633301835224\n",
      "food -0.681971276514\n",
      "good -0.784301962744\n",
      "great -1.17473028272\n"
     ]
    }
   ],
   "source": [
    "print \"10 Coolest Words:\"\n",
    "for word in cool_words[:10]:\n",
    "    print word[1], word[0]\n",
    "\n",
    "print \"\\n10 Least Cool Words:\"\n",
    "for word in cool_words[-10:]:\n",
    "    print word[1], word[0]\n",
    "    \n",
    "print \"\\n10 Funniest Words:\"\n",
    "for word in funny_words[:10]:\n",
    "    print word[1], word[0]\n",
    "\n",
    "print \"\\n10 Least Funny Words:\"\n",
    "for word in funny_words[-10:]:\n",
    "    print word[1], word[0]\n",
    "    \n",
    "print \"\\n10 Most Useful Words:\"\n",
    "for word in useful_words[:10]:\n",
    "    print word[1], word[0]\n",
    "\n",
    "print \"\\n10 Least Useful Words:\"\n",
    "for word in useful_words[-10:]:\n",
    "    print word[1], word[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll run a linear SVC on the data to see how it performs. Performance is very similar to that of the Logistic Regression, though we ultimately opted to go with the latter for reasons explained on the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Useful\n",
      "[[37961 24225]\n",
      " [14053 23761]]\n",
      "False Positive Rate: 0.389557135046\n",
      "False Negative Rate: 0.371634844238\n",
      "True Positive Rate: 0.495165256533\n",
      "True Negative Rate: 0.729822740032\n",
      "Positive Accuracy: 0.620510984993\n",
      "Negative Accuracy: 0.604862843588\n",
      "Cross Validated Accuracy on Sample: 0.606789924084\n",
      "Cross Validated AUC on Sample: 0.648357021728\n",
      "Train Set Accuracy on Sample: 0.61722\n",
      "\n",
      "Funny\n",
      "[[51809 30777]\n",
      " [ 5808 11606]]\n",
      "False Positive Rate: 0.37266606931\n",
      "False Negative Rate: 0.333524750201\n",
      "True Positive Rate: 0.273836207914\n",
      "True Negative Rate: 0.899196417724\n",
      "Positive Accuracy: 0.647524621411\n",
      "Negative Accuracy: 0.628314709203\n",
      "Cross Validated Accuracy on Sample: 0.626159901499\n",
      "Cross Validated AUC on Sample: 0.679360742285\n",
      "Train Set Accuracy on Sample: 0.63415\n",
      "\n",
      "Cool\n",
      "[[50217 27853]\n",
      " [ 8835 13095]]\n",
      "False Positive Rate: 0.356769565774\n",
      "False Negative Rate: 0.402872777018\n",
      "True Positive Rate: 0.319795838625\n",
      "True Negative Rate: 0.850386100386\n",
      "Positive Accuracy: 0.57528499772\n",
      "Negative Accuracy: 0.6432432953\n",
      "Cross Validated Accuracy on Sample: 0.622059980425\n",
      "Cross Validated AUC on Sample: 0.645712321768\n",
      "Train Set Accuracy on Sample: 0.63312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "u_tune = 0.0\n",
    "f_tune = 0.0\n",
    "c_tune = 0.0\n",
    "\n",
    "weights = {0:1/(1 - frac_useful + u_tune), 1:1/(frac_useful - u_tune)}\n",
    "svm = LinearSVC(C=0.001, penalty='l2', class_weight='balanced') #initialize logistic regression model\n",
    "\n",
    "rand = RandomForestClassifier(n_estimators=500, criterion='gini', \n",
    "                              max_features= 17, max_depth=15, class_weight='balanced_subsample')\n",
    "\n",
    "\n",
    "useful_words = test_model(x, y_useful, svm, \"Useful\")\n",
    "funny_words = test_model(x, y_funny, \n",
    "                         svm, \"Funny\")\n",
    "cool_words = test_model(x, y_cool, \n",
    "                        svm, \"Cool\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
