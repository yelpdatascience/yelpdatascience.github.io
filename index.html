<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>CS109a Final Project - User Ratings and Reviews</title>
    <link rel='shortcut icon' href='img/favicon.ico' type='image/x-icon' >

    <!-- CSS libraries -->
    <link href="https://fonts.googleapis.com/css?family=Raleway:500|Roboto" rel="stylesheet">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"
          integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

    <!-- Custom CSS code -->
    <link rel="stylesheet" href="css/style.css">
</head>
<body>

<div class="container-fluid">

    <nav class="site-navigation horizontal-page-navigation">
        <ul class="menu">
            <li><a href="#top">Top</a></li>
            <li><a href="#section1">Introduction</a></li>
            <li><a href="#section2">Data & Visualizations</a></li>
            <li><a href="#section3">Models</a></li>
            <li><a href="#section4">Conclusion</a></li>
            <li><a href="#section5">Works Cited</a></li>
        </ul>
    </nav>


    <div id="top" class="introduction">
        <header>
            <h1>Reviewing Reviews</h1>
            <h2>A Study of the Helpfulness of Yelp User Reviews</h2>
            <h4>John Bowers, Michaela Kane, Jarele Soyinka, & Nisha Swarup</h4>
        </header>
    </div>

    <div id="section1" class="section">
        <h3>Introduction & Objectives</h3>
        <div class="row">
            <div class="col-sm-8">
                <p>
                    Most people are familiar with the popular website <a href="https://www.yelp.com">Yelp</a>, which
                    connects people with local restaurants and businesses and is ranked #51 in US web-traffic.
                    The lifeblood of the website is its extensive reserve of reviews, which allow users to do anything
                    from choosing their entree at dinner to choosing a facility for a medical procedure. In addition to
                    reading about other users' experience at a given location, users can also rate each others' reviews
                    as 'Useful,' 'Funny,' or 'Cool.'
                </p>

                <p>For our project, we decided to investigate exactly how users rated other users’ reviews.
                    In particular, we wanted to investigate what made a post helpful to other users: does word length
                    impact the usefulness of a post? How about the ranking of the restaurant in question? By determining
                    which characteristics of reviews contribute to their usefulness, we can create a model to predict a
                    review's usefulness given certain characteristics. Such a model would help sites like Yelp give its
                    users the best possible experience by highlighting the most useful reviews for a given business.
                    This could also allow companies like Yelp to determine in what order reviews should be shown,
                    rather than showing relatively useless reviews or burying the useful ones under other posts.
                </p>
            </div>
            <div class="col-sm-4">
                <img src="img/Yelp_Logo.png", style="width:300px;height:150px;">
            </div>
        </div>
    </div>

    <div id="section2" class="section">
        <h3>Data Collection & Initial Visualizations</h3>
        <p>
            To start, we downloaded a large archive of Yelp reviews (about 2 million total) from a Yelp-sponsored data
            competition page. To make the data easier to work with, we chose to sample only those reviews which were
            published in 2015 (about 600,000 total). After some simple cleaning to remove duplicates and verify the
            data’s integrity, we began initial data visualization. Our first histogram of a sample of 50,000 reviews
            from the data revealed the following:

        </p>

        <!--Bar graph for frequency of number of votes -->
        <div class="row">
            <h4>Frequency of <span id="type"></span> Votes per Post</h4>
            <h5>Number of Posts with 0 Votes: <span id="zeros"></span></h5>
        </div>

        <div class="row">
            <center><div class="col-sm-8" id="freq-bar"></div></center>
            <div class="col-sm-4">
                <form id="vote-type" onchange="dataManipulation()">
                    <label>View Vote Histogram for: </label><br>
                    <div class="btn-group" data-toggle="buttons">
                        <label class="btn btn-primary active">
                            <input type="radio" name="options" value=0 autocomplete="off" checked> Useful
                        </label>
                        <label class="btn btn-primary">
                            <input type="radio" name="options" value=1 autocomplete="off"> Funny
                        </label>
                        <label class="btn btn-primary">
                            <input type="radio" name="options" value=2 autocomplete="off"> Cool
                        </label>
                    </div>
                </form><br>
            </div>
        </div>


        <p>
            Clearly, many reviews are not receiving any votes for usefulness, coolness, or funniness, which indicates the
            following challenges to consider with this dataset:

        <ol>
        <li>Our model must be capable of dealing with large training and test sets due to the massive volume of reviews published on Yelp</li>
        <li>Due to the nature of Yelp’s review rating system, our training data is inherently flawed – reviews might not receive votes
            because they are:</li>
            <ol type="a">
                <li>actually not useful/funny/cool (this is optimal), or</li>
                <li>despite being useful/funny/cool, the review simply did not receive any exposure and so received zero votes.</li>
            </ol>
        </ol>

        </p>

        <div class="row"><h4>Vote Type Word Clouds</h4></div>

        <div class="row">
            <div class="col-md-12">
                <div class="well">
                    <div id="myCarousel" class="carousel fdi-Carousel slide">
                        <!-- Carousel items -->
                        <div class="carousel fdi-Carousel slide" id="eventCarousel" data-interval="0">
                            <div class="carousel-inner onebyone-carosel">
                                <div class="item active">
                                    <div class="row">
                                        <a href="#" class="thumbnail"><img src="img/all_wordcloud.png" alt="Image" class="img-responsive"
                                                                           style="width: 50%; height: 50%"></a>
                                    </div>
                                    <div class="carousel-caption">
                                        <p>Word Cloud for All Reviews</p>
                                    </div>
                                    <!--/row-->
                                </div>
                                <div class="item">
                                    <div class="row">
                                        <a href="#" class="thumbnail"><img src="img/useful_wordcloud.png" alt="Image" class="img-responsive"
                                                                           style="width: 50%; height: 50%"></a>
                                    </div>
                                    <div class="carousel-caption">
                                        <p>Word Cloud for Useful Reviews</p>
                                    </div>
                                    <!--/row-->
                                </div>
                                <div class="item">
                                    <div class="row">
                                        <a href="#" class="thumbnail"><img src="img/cool_wordcloud.png" alt="Image" class="img-responsive"
                                                                           style="width: 50%; height: 50%"></a>
                                    </div>
                                    <div class="carousel-caption">
                                        <p>Word Cloud for Cool Reviews</p>
                                    </div>
                                    <!--/row-->
                                </div>
                                <div class="item">
                                    <div class="row">
                                        <a href="#" class="thumbnail"><img src="img/funny_wordcloud.png" alt="Image" class="img-responsive"
                                                                           style="width: 50%; height: 50%"></a>
                                    </div>
                                    <div class="carousel-caption">
                                        <p>Word Cloud for Funny Reviews</p>
                                    </div>
                                    <!--/row-->
                                </div>
                            </div>
                            <a class="left carousel-control" href="#eventCarousel" role="button" data-slide="prev">
                                <span class="glyphicon glyphicon-chevron-left" aria-hidden="true"></span>
                                <span class="sr-only">Previous</span>
                            </a>
                            <a class="right carousel-control" href="#eventCarousel" role="button" data-slide="next">
                                <span class="glyphicon glyphicon-chevron-right" aria-hidden="true"></span>
                                <span class="sr-only">Next</span>
                            </a>
                        </div>
                        <!--/carousel-inner-->
                    </div><!--/myCarousel-->
                </div><!--/well-->
            </div>
        </div>

    </div>

    <div id="section3" class="section">
        <h3>Models</h3>
        <p>
            Given that we were working with text data, our first step was to get the reviews into a form interpretable
            by our model. After experimenting with a number of methods to extract features from text, we settled on
            non-binary count vectorization with Tf-idf weighting, a word-based tokenizer, and a no stemming algorithm.
            This configuration seemed to give the best and most stable results overall. We found that using stemming
            algorithms, word pairs, and other enhancements failed to boost our results.
        </p>
        <p>
            After vectorizing the reviews we tried 4 different classification algorithms for sentiment analysis: Random
            Forests, AdaBoost with Random Forests and Logistic Regression, Linear SVC, and Logistic Regression. Each
            algorithm was fit and tested on all three different comment types (funny, cool and useful). Class weights
            were set to “balanced” in each case to compensate for our imbalanced sample: only a minority reviews
            received any votes. Unfortunately, Random Forests and AdaBoost did not perform particularly well on our
            dataset despite intensive tuning. Linear SVC and Logistic Regression performed comparably well, so we
            went on to compare their finer points.
        </p>
        <p>
            Linear SVC seemed more sensitive to its tuning parameter than Logistic Regression. With minor changes to the
            parameter, Linear SVC often fluctuated in accuracy for both classes. Furthermore, as the samples tested
            became larger Linear SVC took significantly longer fit times than Logistic Regression. Finally – and perhaps
            most importantly for our project – Logistic Regression can natively provide class probabilities while making
            predictions. This would be extremely useful in ranking reviews that have not yet been voted on. While SVC is
            capable of making probability estimates, it does so through an extremely expensive cross validation procedure.
            As such, we decided that Logistic Regression provided the best model for our particular objectives.
        </p>

        <!--<div id="tooltip" class="hidden" center = "right">-->
            <!--<p><strong><span id="tip_title"> </span></strong></p>-->
            <!--<p><span id="value">100</span></p>-->
        <!--</div>-->
        <!--<center><div id="bubbles"></div></center>-->

        <!-- Table of Results -->
        <h4>Table of Useful Model Accuracies on Yelp Data</h4>
        <center><table id="results" class="display" cellspacing="0" width="80%">
            <thead>
                <tr>
                    <th>Model</th>
                    <th>False Positive</th>
                    <th>False Negative</th>
                    <th>True Positive</th>
                    <th>True Negative</th>
                    <th>Positive Accuracy</th>
                    <th>Negative Accuracy</th>
                    <th>Cross Validated Accuracy</th>
                    <th>Cross Validated AUC</th>
                    <th>Training Set Accuracy</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Random Forest</td>
                    <td class="best">0.260</td>
                    <td>0.495</td>
                    <td class="best">0.541</td>
                    <td>0.712</td>
                    <td>0.491</td>
                    <td class="best">0.755</td>
                    <td class="best">0.646</td>
                    <td>0.662</td>
                    <td class="best">0.652</td>
                </tr>
                <tr>
                    <td>Linear SVC</td>
                    <td>0.332</td>
                    <td>0.387</td>
                    <td>0.528</td>
                    <td class="best">0.740</td>
                    <td>0.599</td>
                    <td>0.661</td>
                    <td>0.634</td>
                    <td class="best">0.674</td>
                    <td>0.647</td>
                </tr>
                <tr>
                    <td>Logistic Regression</td>
                    <td>0.374</td>
                    <td class="best">0.370</td>
                    <td>0.505</td>
                    <td>0.737</td>
                    <td class="best">0.618</td>
                    <td>0.619</td>
                    <td>0.615</td>
                    <td>0.657</td>
                    <td>0.628</td>
                </tr>
            </tbody>
        </table></center>
        <br>


        <p>
            The green cells indicate the best accuracies for each category.
            It would appear that although Random Forest has some of the best accuracy scores out of the three, it is also
            the most highly varying, with a positive accuracy rate of less than 50%.
            Logistic Regression, on the other hand, tends to be very robust across all of the different scoring methods,
            and it has the highest positive accuracy rate of nearly 62% (compared to Linear SVC's 66.1% and Random Forest's
            49.1%).
        </p>
        <p>
            For the applications of our project, some might argue for prioritizing positive accuracy, since the goal
            of our model is to accurately predict whether a review is useful such that it can be placed where users
            can easily see it. However, when we tried to weight our model on positive accuracy, we found that the
            tradeoff in negative accuracy was unacceptably poor. After all, there is no point in making the useful
            reviews more visible if it also bumps non-useful reviews.
        </p>

        <!-- Table of LogReg Accuracies after Positive Accuracy Weighting -->
        <h4>Useful Logistic Regression Accuracies: Biased in Favor of Positive Accuracy</h4>
        <center><table id="weighted-results" class="display" cellspacing="0" width="80%">
            <thead>
                <tr>
                    <th>Percentage from Balanced</th>
                    <th>Positive Accuracy</th>
                    <th>Change in Positive Accuracy</th>
                    <th>Negative Accuracy</th>
                    <th>Change in Negative Accuracy</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>1%</td>
                    <td>0.672</td>
                    <td> - </td>
                    <td>0.529</td>
                    <td> - </td>
                </tr>
                <tr>
                    <td>2%</td>
                    <td>0.728</td>
                    <td class="inc">+ 0.056</td>
                    <td>0.432</td>
                    <td class="dec">- 0.097</td>
                </tr>
                <tr>
                    <td>3%</td>
                    <td>0.777</td>
                    <td class="inc">+ 0.049</td>
                    <td>0.339</td>
                    <td class="dec">- 0.093</td>
                </tr>
                <tr>
                    <td>4%</td>
                    <td>0.822</td>
                    <td class="inc">+ 0.045</td>
                    <td>0.258</td>
                    <td class="dec">- 0.081</td>
                </tr>
                <tr>
                    <td>5%</td>
                    <td>0.861</td>
                    <td class="inc">+ 0.039</td>
                    <td>0.188</td>
                    <td class="dec">- 0.070</td>
                </tr>
            </tbody>
        </table></center><br>


        <!-- Line Graph of LogReg Accuracies after Positive Accuracy Weighting -->
        <div class="row">
            <center><div class="col-sm-12" id="bias-line"></div></center>
        </div>

        <p>
            Although positive accuracy does increase, negative accuracy decreases at nearly twice the rate, making
            a model biased in favor of positive accuracy unappealing.
        </p>

    </div>

    <div id="section4" class="section">
        <h3>Conclusion</h3>
        <p>Although we feel that we pushed the limits of our models, the confounding factors in our data capped our
            cross validated accuracy in the low to mid 60 percent range for both classes. These factors are inherently
            arbitrary in nature which reflected in the tuning for Random Forests and AdaBoost. Despite, the weighting
            and biases we added to our model, the non-concrete nature of what humans view as funny, cool, or useful
            rendered those efforts useless. Yet the relationship between the review and how it’s perceived is not
            completely arbitrary as evidenced by our success with LinearSVC and Logistic regression. Consistent key
            words and persistent sentiment analysis for reviews proved critical for these models and they were able to
            demonstrate their strengths through their respective cross-validation procedures and class prediction abilities.
        </p>
        <p>
            This is further evidenced as we went beyond the scope of our project to test on Amazon reviews, for which
            the rating system differs. Users on Amazon can rate reviews as either "helpful" or "not helpful." This
            dataset enabled us to target reviews that had been liberally rated, and we secured a large sample pool of helpful
            (voted helpful over 50%) vs. non-helpful reviews. When tested on this dataset, our models achieved positive
            and negative accuracies between 72% and 80%. However, the fact that our model produced meaningful (if
            important) results implies that it would provide a significant improvement over random or chronological
            presentation of voteless reviews.
        </p>
        <p>
            The benefits of a system like this are very apparent. Not only would websites be able to score and rank
            unrated reviews, companies would have a means of seeing common trends on what makes a review helpful. This
            could manifest itself in advertising, with business emphasizing features commonly discussed in reviews,
            product innovation, to meet the needs of that have been previously ignored, and many other areas as valued
            information assets. Even without a perfect accuracy, a wealth of information can always be used to inform
            important decisions.
        </p>
    </div>

    <div id="section5" class="section">
        <h3>Works Cited</h3>
        <br>
        <p>
            Danescu-Niculescu-Mizil, C., Kossinets, G., Kleinberg, J., & Lee, L. (2009, April). How opinions are
            received by online communities: a case study on amazon. com helpfulness votes. In <i>Proceedings of the 18th
            international conference on World wide web</i> (pp. 141-150). ACM.
        </p>
        <br>

        <p>
            Das, S. (2015). Beginners Guide to learn about Content Based Recommender Engines. Retrieved from
            https://www.analyticsvidhya.com/blog/2015/08/beginners-guide-learn-content-based-recommender-systems/
        </p>
        <br>

        <p>
            Ghose, A., & Ipeirotis, P. G. (2011). Estimating the helpfulness and economic impact of product reviews:
            Mining text and reviewer characteristics. <i>IEEE Transactions on Knowledge and Data Engineering</i>, 23(10),
            1498-1512.
        </p>
        <br>

        <p>
            Kaggle (2015). Amazon Fine Food Reviews. Retrieved from
            https://www.kaggle.com/snap/amazon­fine­food­reviews
        </p>
        <br>

        <p>
            Kim, S. M., Pantel, P., Chklovski, T., & Pennacchiotti, M. (2006, July). Automatically assessing review
            helpfulness. <i>In Proceedings of the 2006 Conference on empirical methods in natural language processing</i>
            (pp. 423-430). Association for Computational Linguistics.
        </p>
        <br>

        <p>
            Leskovec, J., Rajaraman, A., & Ullman, J. D. (2014). <i>Mining of massive datasets</i>. Cambridge University
            Press.
        </p>
        <br>

        <p>
            Marafi, S. (2015). Collaborative Filtering with Python. Retrieved from
            http://www.salemmarafi.com/code/collaborative-filtering-with-python/
        </p>
        <br>

        <p>
            McAuley, J. and Leskovec, J. (2013). From amateurs to connoisseurs: modeling the evolution of user expertise through
            online reviews.
        </p>
        <br>

        <p>
            Pang, B., & Lee, L. (2008). Opinion mining and sentiment analysis. <i>Foundations and trends in information
            retrieval</i>, 2(1-2), 1-135.
        </p>
        <br>

        <p>
            Yelp (2016). Yelp Dataset Challenge. Retrieved from
            https://www.yelp.com/dataset_challenge
        </p>
        <br>


        <br> <br>

    </div>
</div>

<!-- Load JS libraries -->
<script src="js/jquery.min.js"></script>
<script src="js/bootstrap.min.js"></script>
<script src="js/d3.min.js"></script>
<script src="js/queue.min.js"></script>

<!--<script type="text/javascript">-->
    <!--jQuery(document).ready(function() {-->

        <!--// Hook up the current state to the nav bar-->
        <!--$('.horizontal-page-navigation').onePageNav();-->

    <!--});-->
<!--</script>-->

<!-- Visualization objects -->
<script src="js/FreqBar.js"></script>
<!--<script src="js/Bubbles.js"></script>-->
<script src="js/BiasLine.js"></script>

<!-- Load data, create visualizations -->
<script src="js/main.js"></script>



</body>
</html>